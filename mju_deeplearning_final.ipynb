{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "D5jpzVScOlss",
    "outputId": "b5cb8683-6175-44b6-b088-628ad4d43501"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install -q tf-nightly-2.0-preview\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "GDKQ3UyEZhjQ",
    "outputId": "3f35832e-b94d-47a8-af18-8326b6a93aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129971 entries, 0 to 129970\n",
      "Data columns (total 14 columns):\n",
      "Unnamed: 0               129971 non-null int64\n",
      "country                  129908 non-null object\n",
      "description              129971 non-null object\n",
      "designation              92506 non-null object\n",
      "points                   129971 non-null int64\n",
      "price                    120975 non-null float64\n",
      "province                 129908 non-null object\n",
      "region_1                 108724 non-null object\n",
      "region_2                 50511 non-null object\n",
      "taster_name              103727 non-null object\n",
      "taster_twitter_handle    98758 non-null object\n",
      "title                    129971 non-null object\n",
      "variety                  129970 non-null object\n",
      "winery                   129971 non-null object\n",
      "dtypes: float64(1), int64(2), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   country                                        description  \\\n",
       "0           0     Italy  Aromas include tropical fruit, broom, brimston...   \n",
       "1           1  Portugal  This is ripe and fruity, a wine that is smooth...   \n",
       "2           2        US  Tart and snappy, the flavors of lime flesh and...   \n",
       "3           3        US  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4           4        US  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                          designation  points  price           province  \\\n",
       "0                        Vulkà Bianco      87    NaN  Sicily & Sardinia   \n",
       "1                            Avidagos      87   15.0              Douro   \n",
       "2                                 NaN      87   14.0             Oregon   \n",
       "3                Reserve Late Harvest      87   13.0           Michigan   \n",
       "4  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n",
       "\n",
       "              region_1           region_2         taster_name  \\\n",
       "0                 Etna                NaN       Kerin O’Keefe   \n",
       "1                  NaN                NaN          Roger Voss   \n",
       "2    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "3  Lake Michigan Shore                NaN  Alexander Peartree   \n",
       "4    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "\n",
       "  taster_twitter_handle                                              title  \\\n",
       "0          @kerinokeefe                  Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1            @vossroger      Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2           @paulgwine       Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3                   NaN  St. Julian 2013 Reserve Late Harvest Riesling ...   \n",
       "4           @paulgwine   Sweet Cheeks 2012 Vintner's Reserve Wild Child...   \n",
       "\n",
       "          variety               winery  \n",
       "0     White Blend              Nicosia  \n",
       "1  Portuguese Red  Quinta dos Avidagos  \n",
       "2      Pinot Gris            Rainstorm  \n",
       "3        Riesling           St. Julian  \n",
       "4      Pinot Noir         Sweet Cheeks  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload data\n",
    "\n",
    "# --- colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# ---\n",
    "\n",
    "import os\n",
    "\n",
    "def load_data():\n",
    "    path = '/Users/jy/Desktop/MJU/4-1/DeepLearning/wine-reviews'\n",
    "    fpath = os.path.join(path, 'winemag-data-130k-v2.csv')\n",
    "    return pd.read_csv(fpath)\n",
    "\n",
    "data = load_data()\n",
    "data.info()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "id": "ZUaG5isCwkkE",
    "outputId": "7ac0646f-726f-47f4-fa2e-3b1129b8af31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129971 entries, 0 to 129970\n",
      "Data columns (total 14 columns):\n",
      "Unnamed: 0               129971 non-null int64\n",
      "country                  129908 non-null object\n",
      "description              129971 non-null object\n",
      "designation              92506 non-null object\n",
      "points                   129971 non-null int64\n",
      "price                    120975 non-null float64\n",
      "province                 129908 non-null object\n",
      "region_1                 108724 non-null object\n",
      "region_2                 50511 non-null object\n",
      "taster_name              103727 non-null object\n",
      "taster_twitter_handle    98758 non-null object\n",
      "title                    129971 non-null object\n",
      "variety                  129970 non-null object\n",
      "winery                   129971 non-null object\n",
      "dtypes: float64(1), int64(2), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   country  ...         variety               winery\n",
       "0           0     Italy  ...     White Blend              Nicosia\n",
       "1           1  Portugal  ...  Portuguese Red  Quinta dos Avidagos\n",
       "2           2        US  ...      Pinot Gris            Rainstorm\n",
       "3           3        US  ...        Riesling           St. Julian\n",
       "4           4        US  ...      Pinot Noir         Sweet Cheeks\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- colab\n",
    "\n",
    "# load data\n",
    "\n",
    "# import io\n",
    "# data = pd.read_csv(io.BytesIO(uploaded['winemag-data-130k-v2.csv']))\n",
    "\n",
    "# data.info()\n",
    "# data.head()\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "FGbSTkCO06Z4",
    "outputId": "1bfb1400-ebb7-420f-b2a0-cf849bf3d675"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>number of reviewers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>1836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84</td>\n",
       "      <td>6480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85</td>\n",
       "      <td>9530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86</td>\n",
       "      <td>12600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>87</td>\n",
       "      <td>16933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88</td>\n",
       "      <td>17207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>89</td>\n",
       "      <td>12226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90</td>\n",
       "      <td>15410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>91</td>\n",
       "      <td>11359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92</td>\n",
       "      <td>9613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>6489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>94</td>\n",
       "      <td>3758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>95</td>\n",
       "      <td>1535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>97</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>98</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    points  number of reviewers\n",
       "0       80                  397\n",
       "1       81                  692\n",
       "2       82                 1836\n",
       "3       83                 3025\n",
       "4       84                 6480\n",
       "5       85                 9530\n",
       "6       86                12600\n",
       "7       87                16933\n",
       "8       88                17207\n",
       "9       89                12226\n",
       "10      90                15410\n",
       "11      91                11359\n",
       "12      92                 9613\n",
       "13      93                 6489\n",
       "14      94                 3758\n",
       "15      95                 1535\n",
       "16      96                  523\n",
       "17      97                  229\n",
       "18      98                   77\n",
       "19      99                   33\n",
       "20     100                   19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating 분포\n",
    "\n",
    "reviews = pd.DataFrame(data.groupby('points').size().rename('number of reviewers').reset_index())\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb2f4f3cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['points'].value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5n4wERO_Ul4M",
    "outputId": "dcb0bd40-84df-4148-e8d1-2b9db1181fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (129971, 14)\n",
      "Total number of examples:  129971\n",
      "Number of examples with the same title and description:  9983\n",
      "Total number of examples:  119988\n"
     ]
    }
   ],
   "source": [
    "# remove duplication\n",
    "\n",
    "print(\"data shape: \", data.shape)\n",
    "print(\"Total number of examples: \", data.shape[0])\n",
    "print(\"Number of examples with the same title and description: \", data[data.duplicated(['description','title'])].shape[0])\n",
    "\n",
    "data=data.drop_duplicates(['description','title'])\n",
    "data=data.reset_index(drop=True)\n",
    "\n",
    "print(\"Total number of examples: \", data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Qd_9X5xl2NKp",
    "outputId": "84261550-bd22-405d-fa5e-532f2d05d506"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  points\n",
       "0  Aromas include tropical fruit, broom, brimston...      87\n",
       "1  This is ripe and fruity, a wine that is smooth...      87\n",
       "2  Tart and snappy, the flavors of lime flesh and...      87\n",
       "3  Pineapple rind, lemon pith and orange blossom ...      87\n",
       "4  Much like the regular bottling from 2012, this...      87"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only the neccessary columns\n",
    "permanent = data[['description', 'points']]\n",
    "permanent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uebjIkoUg1ze",
    "outputId": "53305e95-cf6c-45d2-e002-906780033940"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessing_corpus(corpus):\n",
    "    corpus = corpus.str.lower()\n",
    "    corpus = corpus.str.replace(r'[^a-z0-9\\s]', ' ', regex=True)\n",
    "    return corpus.values.tolist()\n",
    "\n",
    "def making_vocab(corpus, top_n_ratio=1.0):\n",
    "    words = np.concatenate(np.core.defchararray.split(corpus)).tolist()\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stopWords]\n",
    "\n",
    "    counter = Counter(words)\n",
    "    if top_n_ratio is not 1.0:\n",
    "        counter = Counter(dict(counter.most_common(int(top_n_ratio*len(counter)))))\n",
    "    unique_words = list(counter) + ['UNK']\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "qvS9tm2alsyh",
    "outputId": "2336dbba-0d39-4f2c-b324-12aa2d3bfc64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aromas include tropical fruit  broom  brimstone and dried herb  the palate isn t overly expressive  offering unripened apple  citrus and dried sage alongside brisk acidity \n",
      "<class 'list'>\n",
      "['wine', 'flavors', 'fruit', 'aromas', 'palate', 'finish', 'acidity', 'tannins', 'drink', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocessing_corpus(permanent['description'])\n",
    "print(corpus[0])\n",
    "print(type(corpus))\n",
    "\n",
    "vocab = making_vocab(corpus, top_n_ratio=0.8)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "KDzEpc2LsOWL",
    "outputId": "2cace9cc-3822-440d-d1a9-07996596c5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wine':  1 , ...}\n",
      "vocab size:  24771\n"
     ]
    }
   ],
   "source": [
    "def vocab_indexing(vocab):\n",
    "    word2index = { word:(index + 1) for index, word in enumerate(vocab) }\n",
    "    return word2index\n",
    "\n",
    "word2index = vocab_indexing(vocab)\n",
    "\n",
    "vocab_size = len(word2index)\n",
    "print(\"{'wine': \", word2index['wine'], \", ...}\")\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8sJzQZWtlhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119988\n",
      "<class 'list'>\n",
      "[4, 786, 159, 3, 1942, 3661, 0, 57, 72, 0, 5, 0, 0, 933, 839, 256, 13541, 28, 37, 0, 57, 360, 104, 308, 7]\n"
     ]
    }
   ],
   "source": [
    "def word_index_into_corpus(word2index, corpus):\n",
    "    indexed_corpus = []\n",
    "    for doc in corpus:\n",
    "        indexed_corpus.append([word2index[word] if word in word2index else 0 for word in doc.split()])\n",
    "    return indexed_corpus\n",
    "\n",
    "indexed_corpus = word_index_into_corpus(word2index, corpus)\n",
    "print(len(indexed_corpus))\n",
    "print(type(indexed_corpus))\n",
    "print(indexed_corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hs6HsSo7uoTJ",
    "outputId": "4c69a1fe-007e-4eef-c90b-636cdb7959f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "# get max length\n",
    "maxValue = 0\n",
    "for idx, sentance in enumerate(indexed_corpus):\n",
    "  if maxValue < len(indexed_corpus[idx]):\n",
    "    maxValue = len(indexed_corpus[idx])\n",
    "    \n",
    "print(maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DtcKDT2KvINK",
    "outputId": "2b725daa-2c8b-47c4-b343-d377d8710211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# get min length\n",
    "minValue = 138\n",
    "for idx, sentance in enumerate(indexed_corpus):\n",
    "  if minValue > len(indexed_corpus[idx]):\n",
    "    minValue = len(indexed_corpus[idx])\n",
    "    \n",
    "print(minValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wumFmEVavQGu"
   },
   "outputs": [],
   "source": [
    "# add padding\n",
    "maxlen = 138\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(indexed_corpus,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iP1VHGNHC6AY",
    "outputId": "92795e1f-eee5-4435-9940-0ea1646039ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119988, 138)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2bb6A4FiwKlk",
    "outputId": "b45ac5c6-060c-4002-bb25-27b6808418ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# train_data, train_labels\n",
    "train_labels = []\n",
    "\n",
    "for index, point in enumerate(permanent['points']):\n",
    "    if point < 84:\n",
    "        train_labels.append(0)\n",
    "    elif point >= 84 and point < 88:\n",
    "        train_labels.append(1)\n",
    "    elif point >= 88 and point < 92:\n",
    "        train_labels.append(2)\n",
    "    elif point >= 92 and point < 96:\n",
    "        train_labels.append(3)\n",
    "    else:\n",
    "        train_labels.append(4)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "yRI70dqX16rh",
    "outputId": "f1f8c0d8-6b7e-41eb-e98b-ff53da3f6997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4   786   159 ...     0     0     0]\n",
      " [    0     0    12 ...     0     0     0]\n",
      " [   77     0   844 ...     0     0     0]\n",
      " ...\n",
      " [   27 15592  1486 ...     0     0     0]\n",
      " [    0    19    69 ...     0     0     0]\n",
      " [  140    17     0 ...     0     0     0]]\n",
      "119988\n",
      "[1 1 1 ... 2 2 2]\n",
      "119988\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(len(train_data))\n",
    "\n",
    "print(train_labels)\n",
    "print(len(train_labels))\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "fAlDwot8vZqR",
    "outputId": "61027e5c-8c89-40ba-d3c0-e77de5dbae4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jy/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 138, 16)           396336    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 396,693\n",
      "Trainable params: 396,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=maxValue),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "id": "DriiSlmLv6kz",
    "outputId": "1d6147e1-f1dc-441c-b5a0-2050704758a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/30\n",
      "95990/95990 [==============================] - 3s 31us/sample - loss: 1.3697 - accuracy: 0.4240 - val_loss: 1.2628 - val_accuracy: 0.4192\n",
      "Epoch 2/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 1.1606 - accuracy: 0.4792 - val_loss: 1.1839 - val_accuracy: 0.5519\n",
      "Epoch 3/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 1.0522 - accuracy: 0.5842 - val_loss: 1.0426 - val_accuracy: 0.5711\n",
      "Epoch 4/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.9282 - accuracy: 0.6124 - val_loss: 0.9424 - val_accuracy: 0.6090\n",
      "Epoch 5/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.8526 - accuracy: 0.6430 - val_loss: 0.8837 - val_accuracy: 0.6329\n",
      "Epoch 6/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.8034 - accuracy: 0.6607 - val_loss: 0.8439 - val_accuracy: 0.6412\n",
      "Epoch 7/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.7681 - accuracy: 0.6710 - val_loss: 0.8085 - val_accuracy: 0.6507\n",
      "Epoch 8/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.7409 - accuracy: 0.6782 - val_loss: 0.7860 - val_accuracy: 0.6546\n",
      "Epoch 9/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.7193 - accuracy: 0.6850 - val_loss: 0.7689 - val_accuracy: 0.6609\n",
      "Epoch 10/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.7023 - accuracy: 0.6955 - val_loss: 0.7547 - val_accuracy: 0.6680\n",
      "Epoch 11/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6875 - accuracy: 0.7016 - val_loss: 0.7436 - val_accuracy: 0.6741\n",
      "Epoch 12/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6750 - accuracy: 0.7062 - val_loss: 0.7356 - val_accuracy: 0.6769\n",
      "Epoch 13/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6640 - accuracy: 0.7099 - val_loss: 0.7266 - val_accuracy: 0.6792\n",
      "Epoch 14/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6540 - accuracy: 0.7146 - val_loss: 0.7198 - val_accuracy: 0.6807\n",
      "Epoch 15/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6452 - accuracy: 0.7179 - val_loss: 0.7157 - val_accuracy: 0.6819\n",
      "Epoch 16/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6373 - accuracy: 0.7220 - val_loss: 0.7135 - val_accuracy: 0.6842\n",
      "Epoch 17/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6307 - accuracy: 0.7250 - val_loss: 0.7148 - val_accuracy: 0.6834\n",
      "Epoch 18/30\n",
      "95990/95990 [==============================] - 3s 29us/sample - loss: 0.6239 - accuracy: 0.7287 - val_loss: 0.7115 - val_accuracy: 0.6851\n",
      "Epoch 19/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6179 - accuracy: 0.7310 - val_loss: 0.7114 - val_accuracy: 0.6855\n",
      "Epoch 20/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6124 - accuracy: 0.7337 - val_loss: 0.7108 - val_accuracy: 0.6876\n",
      "Epoch 21/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6073 - accuracy: 0.7369 - val_loss: 0.7130 - val_accuracy: 0.6871\n",
      "Epoch 22/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.6025 - accuracy: 0.7387 - val_loss: 0.7093 - val_accuracy: 0.6885\n",
      "Epoch 23/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5977 - accuracy: 0.7415 - val_loss: 0.7120 - val_accuracy: 0.6881\n",
      "Epoch 24/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5927 - accuracy: 0.7447 - val_loss: 0.7088 - val_accuracy: 0.6888\n",
      "Epoch 25/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5884 - accuracy: 0.7465 - val_loss: 0.7087 - val_accuracy: 0.6881\n",
      "Epoch 26/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5847 - accuracy: 0.7487 - val_loss: 0.7171 - val_accuracy: 0.6863\n",
      "Epoch 27/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5808 - accuracy: 0.7511 - val_loss: 0.7127 - val_accuracy: 0.6889\n",
      "Epoch 28/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5761 - accuracy: 0.7522 - val_loss: 0.7139 - val_accuracy: 0.6885\n",
      "Epoch 29/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5727 - accuracy: 0.7549 - val_loss: 0.7203 - val_accuracy: 0.6870\n",
      "Epoch 30/30\n",
      "95990/95990 [==============================] - 3s 28us/sample - loss: 0.5688 - accuracy: 0.7567 - val_loss: 0.7175 - val_accuracy: 0.6866\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "HzOvBVQCIiYT",
    "outputId": "6c1c9ac2-559a-4f88-e09a-54cc026320f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 138, 16)           396336    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               41472     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 438,453\n",
      "Trainable params: 438,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "  layers.Embedding(vocab_size, embedding_dim, input_length=maxValue),\n",
    "  layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "  layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "BqtIPnTaJAtA",
    "outputId": "17754a0c-d952-4932-d659-8e365232255c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 231s 2ms/sample - loss: 1.1751 - accuracy: 0.4622 - val_loss: 0.9743 - val_accuracy: 0.5549\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 226s 2ms/sample - loss: 0.8408 - accuracy: 0.6279 - val_loss: 0.7971 - val_accuracy: 0.6380\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 224s 2ms/sample - loss: 0.7473 - accuracy: 0.6725 - val_loss: 0.7507 - val_accuracy: 0.6697\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 225s 2ms/sample - loss: 0.6996 - accuracy: 0.6961 - val_loss: 0.7493 - val_accuracy: 0.6703\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 223s 2ms/sample - loss: 0.6699 - accuracy: 0.7087 - val_loss: 0.7456 - val_accuracy: 0.6723\n",
      "Epoch 6/10\n",
      "95990/95990 [==============================] - 222s 2ms/sample - loss: 0.6564 - accuracy: 0.7186 - val_loss: 0.7371 - val_accuracy: 0.6765\n",
      "Epoch 7/10\n",
      "95990/95990 [==============================] - 218s 2ms/sample - loss: 0.6353 - accuracy: 0.7278 - val_loss: 0.7370 - val_accuracy: 0.6808\n",
      "Epoch 8/10\n",
      "95990/95990 [==============================] - 217s 2ms/sample - loss: 0.6139 - accuracy: 0.7388 - val_loss: 0.7421 - val_accuracy: 0.6811\n",
      "Epoch 9/10\n",
      "95990/95990 [==============================] - 218s 2ms/sample - loss: 0.5967 - accuracy: 0.7474 - val_loss: 0.7389 - val_accuracy: 0.6800\n",
      "Epoch 10/10\n",
      " 1536/95990 [..............................] - ETA: 3:08 - loss: 0.5642 - accuracy: 0.7624"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "cPaV5BxLSrcU",
    "outputId": "3e26d507-cc73-4e3a-ac70-3e18d5c6b38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# upload data & load the whole embedding into memory\n",
    "\n",
    "glove_path = '/Users/jy/Desktop/MJU/4-1/DeepLearning/glove.6B'\n",
    "glove_fpath = os.path.join(glove_path, 'glove.6B.100d.txt')\n",
    "f = open(glove_fpath)\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UGgVTMjZXKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47652999  0.42094001  0.17423999 ... -0.54451001  0.80343002\n",
      "   0.50082999]\n",
      " [-0.52666998  0.64282     0.26199999 ... -0.71044999  0.33175999\n",
      "   0.85721999]\n",
      " [-0.86568999  0.48032999 -0.39666    ... -0.69826001  1.19219995\n",
      "   0.41204   ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(24771, 100)\n",
      "not_in_glove:  4030\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "# vocab_size: 24771\n",
    "\n",
    "not_in_glove = 0\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for index, word in enumerate(vocab):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        not_in_glove += 1\n",
    "\n",
    "print(embedding_matrix)\n",
    "print(embedding_matrix.shape)\n",
    "print('not_in_glove: ', not_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,562,225\n",
      "Trainable params: 2,562,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue\n",
    ")\n",
    "\n",
    "model3 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 276s 3ms/sample - loss: 1.0912 - acc: 0.4994 - val_loss: 0.9218 - val_acc: 0.5903\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 260s 3ms/sample - loss: 0.8309 - acc: 0.6307 - val_loss: 0.7832 - val_acc: 0.6543\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 265s 3ms/sample - loss: 0.7405 - acc: 0.6753 - val_loss: 0.7433 - val_acc: 0.6714\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 275s 3ms/sample - loss: 0.6966 - acc: 0.6963 - val_loss: 0.7280 - val_acc: 0.6823\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 279s 3ms/sample - loss: 0.6636 - acc: 0.7120 - val_loss: 0.7196 - val_acc: 0.6838\n",
      "Epoch 6/10\n",
      "95990/95990 [==============================] - 263s 3ms/sample - loss: 0.6382 - acc: 0.7256 - val_loss: 0.7219 - val_acc: 0.6851\n",
      "Epoch 7/10\n",
      "95990/95990 [==============================] - 262s 3ms/sample - loss: 0.6161 - acc: 0.7356 - val_loss: 0.7308 - val_acc: 0.6824\n",
      "Epoch 8/10\n",
      "36864/95990 [==========>...................] - ETA: 2:45 - loss: 0.5869 - acc: 0.7514"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9defca8d2e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 138, 64)           32064     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 138, 32)           2080      \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 2,569,489\n",
      "Trainable params: 2,569,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue\n",
    ")\n",
    "\n",
    "model4 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model4.summary()\n",
    "model4.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 247s 3ms/sample - loss: 1.1517 - acc: 0.4621 - val_loss: 0.9698 - val_acc: 0.5752\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 245s 3ms/sample - loss: 0.8532 - acc: 0.6221 - val_loss: 0.7887 - val_acc: 0.6483\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 265s 3ms/sample - loss: 0.7607 - acc: 0.6642 - val_loss: 0.7762 - val_acc: 0.6615\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 244s 3ms/sample - loss: 0.7165 - acc: 0.6862 - val_loss: 0.7487 - val_acc: 0.6761\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 258s 3ms/sample - loss: 0.6913 - acc: 0.7006 - val_loss: 0.7370 - val_acc: 0.6784\n",
      "Epoch 6/10\n",
      " 8704/95990 [=>............................] - ETA: 3:45 - loss: 0.6613 - acc: 0.7160"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-37f6af84dda9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 138, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 69, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 69, 32)            2080      \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 128)               41472     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 2,563,873\n",
      "Trainable params: 2,563,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue\n",
    ")\n",
    "\n",
    "model5 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model5.summary()\n",
    "model5.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history5.history['acc'], label='train_acc')\n",
    "plt.plot(history5.history[\"val_acc\"], label='validation_acc')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/200\n",
      "95990/95990 [==============================] - 149s 2ms/sample - loss: 1.1030 - acc: 0.4916 - val_loss: 0.9020 - val_acc: 0.5963\n",
      "Epoch 2/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.8231 - acc: 0.6333 - val_loss: 0.7877 - val_acc: 0.6544\n",
      "Epoch 3/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.7493 - acc: 0.6688 - val_loss: 0.7727 - val_acc: 0.6603\n",
      "Epoch 4/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.7029 - acc: 0.6925 - val_loss: 0.7541 - val_acc: 0.6681\n",
      "Epoch 5/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.6741 - acc: 0.7066 - val_loss: 0.7479 - val_acc: 0.6724\n",
      "Epoch 6/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.6464 - acc: 0.7218 - val_loss: 0.8433 - val_acc: 0.6408\n",
      "Epoch 7/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.6285 - acc: 0.7324 - val_loss: 0.7600 - val_acc: 0.6703\n",
      "Epoch 8/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.5935 - acc: 0.7493 - val_loss: 0.7755 - val_acc: 0.6675\n",
      "Epoch 9/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.5708 - acc: 0.7608 - val_loss: 0.8125 - val_acc: 0.6596\n",
      "Epoch 10/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.5454 - acc: 0.7737 - val_loss: 0.8236 - val_acc: 0.6619\n",
      "Epoch 11/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.5169 - acc: 0.7882 - val_loss: 0.8437 - val_acc: 0.6616\n",
      "Epoch 12/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.4872 - acc: 0.8039 - val_loss: 0.8912 - val_acc: 0.6560\n",
      "Epoch 13/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.4502 - acc: 0.8214 - val_loss: 0.9497 - val_acc: 0.6486\n",
      "Epoch 14/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.4159 - acc: 0.8373 - val_loss: 0.9720 - val_acc: 0.6486\n",
      "Epoch 15/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.3869 - acc: 0.8509 - val_loss: 1.0900 - val_acc: 0.6339\n",
      "Epoch 16/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.3577 - acc: 0.8650 - val_loss: 1.1391 - val_acc: 0.6416\n",
      "Epoch 17/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.3250 - acc: 0.8768 - val_loss: 1.1789 - val_acc: 0.6266\n",
      "Epoch 18/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.2891 - acc: 0.8931 - val_loss: 1.3374 - val_acc: 0.6224\n",
      "Epoch 19/200\n",
      "95990/95990 [==============================] - 140s 1ms/sample - loss: 0.2684 - acc: 0.9009 - val_loss: 1.3296 - val_acc: 0.6252\n",
      "Epoch 20/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.2546 - acc: 0.9060 - val_loss: 1.4429 - val_acc: 0.6271\n",
      "Epoch 21/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.2135 - acc: 0.9241 - val_loss: 1.5114 - val_acc: 0.6274\n",
      "Epoch 22/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.1962 - acc: 0.9303 - val_loss: 1.6594 - val_acc: 0.6262\n",
      "Epoch 23/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.1910 - acc: 0.9313 - val_loss: 1.5908 - val_acc: 0.6073\n",
      "Epoch 24/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.1672 - acc: 0.9425 - val_loss: 1.7942 - val_acc: 0.6210\n",
      "Epoch 25/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1453 - acc: 0.9498 - val_loss: 1.8370 - val_acc: 0.6178\n",
      "Epoch 26/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1406 - acc: 0.9514 - val_loss: 1.8427 - val_acc: 0.6216\n",
      "Epoch 27/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1253 - acc: 0.9577 - val_loss: 2.1009 - val_acc: 0.6186\n",
      "Epoch 28/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1149 - acc: 0.9611 - val_loss: 2.1294 - val_acc: 0.6173\n",
      "Epoch 29/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1180 - acc: 0.9600 - val_loss: 2.1386 - val_acc: 0.6128\n",
      "Epoch 30/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0995 - acc: 0.9671 - val_loss: 2.2475 - val_acc: 0.6056\n",
      "Epoch 31/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.1071 - acc: 0.9632 - val_loss: 1.9960 - val_acc: 0.6132\n",
      "Epoch 32/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0878 - acc: 0.9713 - val_loss: 2.2525 - val_acc: 0.6142\n",
      "Epoch 33/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0819 - acc: 0.9728 - val_loss: 2.4794 - val_acc: 0.6053\n",
      "Epoch 34/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0828 - acc: 0.9719 - val_loss: 2.5027 - val_acc: 0.5986\n",
      "Epoch 35/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0708 - acc: 0.9772 - val_loss: 2.5035 - val_acc: 0.6058\n",
      "Epoch 36/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0636 - acc: 0.9795 - val_loss: 2.7627 - val_acc: 0.6055\n",
      "Epoch 37/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0629 - acc: 0.9795 - val_loss: 2.6601 - val_acc: 0.6000\n",
      "Epoch 38/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0609 - acc: 0.9804 - val_loss: 2.5297 - val_acc: 0.6058\n",
      "Epoch 39/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0840 - acc: 0.9710 - val_loss: 2.4377 - val_acc: 0.5968\n",
      "Epoch 40/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0535 - acc: 0.9835 - val_loss: 2.7115 - val_acc: 0.5982\n",
      "Epoch 41/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0470 - acc: 0.9853 - val_loss: 2.8712 - val_acc: 0.6063\n",
      "Epoch 42/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.1010 - acc: 0.9662 - val_loss: 2.5529 - val_acc: 0.6044\n",
      "Epoch 43/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0427 - acc: 0.9874 - val_loss: 2.6545 - val_acc: 0.6078\n",
      "Epoch 44/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0371 - acc: 0.9889 - val_loss: 2.9130 - val_acc: 0.6082\n",
      "Epoch 45/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0357 - acc: 0.9894 - val_loss: 2.9976 - val_acc: 0.6073\n",
      "Epoch 46/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0359 - acc: 0.9891 - val_loss: 3.1150 - val_acc: 0.6079\n",
      "Epoch 47/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.2493 - acc: 0.9111 - val_loss: 2.0459 - val_acc: 0.6073\n",
      "Epoch 48/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0928 - acc: 0.9670 - val_loss: 2.4027 - val_acc: 0.6048\n",
      "Epoch 49/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0527 - acc: 0.9827 - val_loss: 2.6526 - val_acc: 0.6042\n",
      "Epoch 50/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0442 - acc: 0.9857 - val_loss: 2.7936 - val_acc: 0.6053\n",
      "Epoch 51/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0383 - acc: 0.9878 - val_loss: 2.9746 - val_acc: 0.6023\n",
      "Epoch 52/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0329 - acc: 0.9896 - val_loss: 2.9956 - val_acc: 0.6006\n",
      "Epoch 53/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0360 - acc: 0.9885 - val_loss: 3.0865 - val_acc: 0.6029\n",
      "Epoch 54/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0481 - acc: 0.9840 - val_loss: 2.9895 - val_acc: 0.5949\n",
      "Epoch 55/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0392 - acc: 0.9873 - val_loss: 2.9911 - val_acc: 0.5996\n",
      "Epoch 56/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0283 - acc: 0.9914 - val_loss: 3.1307 - val_acc: 0.6074\n",
      "Epoch 57/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0278 - acc: 0.9913 - val_loss: 3.3564 - val_acc: 0.6036\n",
      "Epoch 58/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0411 - acc: 0.9862 - val_loss: 3.1148 - val_acc: 0.6035\n",
      "Epoch 59/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0323 - acc: 0.9895 - val_loss: 3.3032 - val_acc: 0.5948\n",
      "Epoch 60/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0284 - acc: 0.9912 - val_loss: 3.2500 - val_acc: 0.6035\n",
      "Epoch 61/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0242 - acc: 0.9925 - val_loss: 3.2639 - val_acc: 0.6055\n",
      "Epoch 62/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0261 - acc: 0.9919 - val_loss: 3.2503 - val_acc: 0.6051\n",
      "Epoch 63/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0542 - acc: 0.9825 - val_loss: 2.6413 - val_acc: 0.6040\n",
      "Epoch 64/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0389 - acc: 0.9875 - val_loss: 3.0165 - val_acc: 0.6083\n",
      "Epoch 65/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0197 - acc: 0.9939 - val_loss: 3.3024 - val_acc: 0.6053\n",
      "Epoch 66/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0174 - acc: 0.9945 - val_loss: 3.2659 - val_acc: 0.6060\n",
      "Epoch 67/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 3.3026 - val_acc: 0.6102\n",
      "Epoch 68/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0220 - acc: 0.9931 - val_loss: 3.4985 - val_acc: 0.6068\n",
      "Epoch 69/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0273 - acc: 0.9910 - val_loss: 3.1403 - val_acc: 0.6050\n",
      "Epoch 70/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0259 - acc: 0.9915 - val_loss: 3.2791 - val_acc: 0.6018\n",
      "Epoch 71/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0192 - acc: 0.9940 - val_loss: 3.3793 - val_acc: 0.6051\n",
      "Epoch 72/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0247 - acc: 0.9919 - val_loss: 3.4891 - val_acc: 0.6053\n",
      "Epoch 73/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0263 - acc: 0.9914 - val_loss: 3.2043 - val_acc: 0.6001\n",
      "Epoch 74/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0221 - acc: 0.9931 - val_loss: 3.3963 - val_acc: 0.6007\n",
      "Epoch 75/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0192 - acc: 0.9937 - val_loss: 3.3486 - val_acc: 0.6029\n",
      "Epoch 76/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0155 - acc: 0.9950 - val_loss: 3.6907 - val_acc: 0.6026\n",
      "Epoch 77/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 3.3121 - val_acc: 0.6025\n",
      "Epoch 78/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0231 - acc: 0.9924 - val_loss: 3.3659 - val_acc: 0.6005\n",
      "Epoch 79/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0210 - acc: 0.9932 - val_loss: 3.3728 - val_acc: 0.5988\n",
      "Epoch 80/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 3.4589 - val_acc: 0.5968\n",
      "Epoch 81/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0132 - acc: 0.9957 - val_loss: 3.5935 - val_acc: 0.6046\n",
      "Epoch 82/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0186 - acc: 0.9938 - val_loss: 3.5760 - val_acc: 0.6060\n",
      "Epoch 83/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 3.4609 - val_acc: 0.6083\n",
      "Epoch 84/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0250 - acc: 0.9915 - val_loss: 3.4205 - val_acc: 0.6061\n",
      "Epoch 85/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0179 - acc: 0.9941 - val_loss: 3.4837 - val_acc: 0.6019\n",
      "Epoch 86/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0225 - acc: 0.9924 - val_loss: 3.4544 - val_acc: 0.6034\n",
      "Epoch 87/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0183 - acc: 0.9939 - val_loss: 3.4100 - val_acc: 0.6057\n",
      "Epoch 88/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0132 - acc: 0.9955 - val_loss: 3.5530 - val_acc: 0.6078\n",
      "Epoch 89/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0153 - acc: 0.9950 - val_loss: 3.5275 - val_acc: 0.6081\n",
      "Epoch 90/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0140 - acc: 0.9955 - val_loss: 3.5700 - val_acc: 0.6051\n",
      "Epoch 91/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0199 - acc: 0.9934 - val_loss: 3.5336 - val_acc: 0.6083\n",
      "Epoch 92/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0181 - acc: 0.9939 - val_loss: 3.5762 - val_acc: 0.6013\n",
      "Epoch 93/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0186 - acc: 0.9939 - val_loss: 3.5688 - val_acc: 0.5943\n",
      "Epoch 94/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0428 - acc: 0.9866 - val_loss: 2.8140 - val_acc: 0.6052\n",
      "Epoch 95/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0184 - acc: 0.9942 - val_loss: 3.2779 - val_acc: 0.6086\n",
      "Epoch 96/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0084 - acc: 0.9974 - val_loss: 3.6263 - val_acc: 0.6067\n",
      "Epoch 97/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 3.8111 - val_acc: 0.6002\n",
      "Epoch 98/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0083 - acc: 0.9971 - val_loss: 3.6628 - val_acc: 0.5988\n",
      "Epoch 99/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0116 - acc: 0.9960 - val_loss: 3.7398 - val_acc: 0.6066\n",
      "Epoch 100/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0179 - acc: 0.9940 - val_loss: 3.4732 - val_acc: 0.6038\n",
      "Epoch 101/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0160 - acc: 0.9949 - val_loss: 3.5657 - val_acc: 0.6053\n",
      "Epoch 102/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0219 - acc: 0.9927 - val_loss: 3.4230 - val_acc: 0.6045\n",
      "Epoch 103/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 3.5842 - val_acc: 0.6116\n",
      "Epoch 104/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0084 - acc: 0.9974 - val_loss: 3.6597 - val_acc: 0.6048\n",
      "Epoch 105/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0094 - acc: 0.9968 - val_loss: 3.7804 - val_acc: 0.6033\n",
      "Epoch 106/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0118 - acc: 0.9962 - val_loss: 3.6838 - val_acc: 0.6070\n",
      "Epoch 107/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0115 - acc: 0.9961 - val_loss: 3.7764 - val_acc: 0.6086\n",
      "Epoch 108/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0165 - acc: 0.9944 - val_loss: 3.6236 - val_acc: 0.6068\n",
      "Epoch 109/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0983 - acc: 0.9718 - val_loss: 2.6873 - val_acc: 0.6077\n",
      "Epoch 110/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0138 - acc: 0.9959 - val_loss: 3.0293 - val_acc: 0.6060\n",
      "Epoch 111/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0059 - acc: 0.9984 - val_loss: 3.3267 - val_acc: 0.6053\n",
      "Epoch 112/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0061 - acc: 0.9984 - val_loss: 3.5096 - val_acc: 0.6075\n",
      "Epoch 113/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 3.6377 - val_acc: 0.6048\n",
      "Epoch 114/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 3.7318 - val_acc: 0.6076\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 3.7656 - val_acc: 0.6076\n",
      "Epoch 116/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 3.8051 - val_acc: 0.6058\n",
      "Epoch 117/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0082 - acc: 0.9973 - val_loss: 3.7619 - val_acc: 0.6063\n",
      "Epoch 118/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0187 - acc: 0.9936 - val_loss: 3.4473 - val_acc: 0.6075\n",
      "Epoch 119/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0236 - acc: 0.9922 - val_loss: 3.3142 - val_acc: 0.6061\n",
      "Epoch 120/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0117 - acc: 0.9962 - val_loss: 3.6251 - val_acc: 0.6010\n",
      "Epoch 121/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 3.6032 - val_acc: 0.6101\n",
      "Epoch 122/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 3.6527 - val_acc: 0.6047\n",
      "Epoch 123/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0096 - acc: 0.9969 - val_loss: 3.7989 - val_acc: 0.6033\n",
      "Epoch 124/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0126 - acc: 0.9956 - val_loss: 3.5457 - val_acc: 0.6043\n",
      "Epoch 125/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0162 - acc: 0.9944 - val_loss: 3.6373 - val_acc: 0.6028\n",
      "Epoch 126/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 3.7026 - val_acc: 0.6049\n",
      "Epoch 127/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0093 - acc: 0.9971 - val_loss: 3.7560 - val_acc: 0.6037\n",
      "Epoch 128/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 3.8457 - val_acc: 0.6081\n",
      "Epoch 129/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 3.7815 - val_acc: 0.6038\n",
      "Epoch 130/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0113 - acc: 0.9964 - val_loss: 3.6365 - val_acc: 0.6064\n",
      "Epoch 131/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0323 - acc: 0.9895 - val_loss: 3.3730 - val_acc: 0.6068\n",
      "Epoch 132/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0081 - acc: 0.9975 - val_loss: 3.6013 - val_acc: 0.6084\n",
      "Epoch 133/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0055 - acc: 0.9983 - val_loss: 3.8939 - val_acc: 0.6046\n",
      "Epoch 134/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0061 - acc: 0.9982 - val_loss: 3.7842 - val_acc: 0.6106\n",
      "Epoch 135/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0041 - acc: 0.9987 - val_loss: 3.8678 - val_acc: 0.6138\n",
      "Epoch 136/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0096 - acc: 0.9969 - val_loss: 3.8172 - val_acc: 0.6099\n",
      "Epoch 137/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0132 - acc: 0.9957 - val_loss: 3.7337 - val_acc: 0.6086\n",
      "Epoch 138/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0152 - acc: 0.9951 - val_loss: 3.6181 - val_acc: 0.6076\n",
      "Epoch 139/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0116 - acc: 0.9962 - val_loss: 3.5954 - val_acc: 0.6066\n",
      "Epoch 140/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 3.6402 - val_acc: 0.6097\n",
      "Epoch 141/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0064 - acc: 0.9978 - val_loss: 3.8148 - val_acc: 0.6086\n",
      "Epoch 142/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0091 - acc: 0.9971 - val_loss: 3.7374 - val_acc: 0.6067\n",
      "Epoch 143/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0113 - acc: 0.9961 - val_loss: 3.7746 - val_acc: 0.6054\n",
      "Epoch 144/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0101 - acc: 0.9965 - val_loss: 3.7597 - val_acc: 0.6080\n",
      "Epoch 145/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0165 - acc: 0.9946 - val_loss: 3.6087 - val_acc: 0.6043\n",
      "Epoch 146/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 3.6069 - val_acc: 0.6086\n",
      "Epoch 147/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0074 - acc: 0.9976 - val_loss: 3.7405 - val_acc: 0.6097\n",
      "Epoch 148/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0052 - acc: 0.9983 - val_loss: 3.8217 - val_acc: 0.6079\n",
      "Epoch 149/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0060 - acc: 0.9980 - val_loss: 3.9103 - val_acc: 0.6098\n",
      "Epoch 150/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 3.8493 - val_acc: 0.6041\n",
      "Epoch 151/200\n",
      "95990/95990 [==============================] - 144s 2ms/sample - loss: 0.0164 - acc: 0.9944 - val_loss: 3.6495 - val_acc: 0.5961\n",
      "Epoch 152/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0131 - acc: 0.9957 - val_loss: 3.5573 - val_acc: 0.6067\n",
      "Epoch 153/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0072 - acc: 0.9977 - val_loss: 3.8014 - val_acc: 0.6029\n",
      "Epoch 154/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 3.8011 - val_acc: 0.6014\n",
      "Epoch 155/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0097 - acc: 0.9970 - val_loss: 3.8221 - val_acc: 0.6028\n",
      "Epoch 156/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0093 - acc: 0.9969 - val_loss: 3.8521 - val_acc: 0.6061\n",
      "Epoch 157/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 3.7273 - val_acc: 0.6119\n",
      "Epoch 158/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0138 - acc: 0.9955 - val_loss: 3.6293 - val_acc: 0.6057\n",
      "Epoch 159/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 3.6540 - val_acc: 0.6112\n",
      "Epoch 160/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 3.7705 - val_acc: 0.6101\n",
      "Epoch 161/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 3.8330 - val_acc: 0.6107\n",
      "Epoch 162/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0086 - acc: 0.9974 - val_loss: 3.7163 - val_acc: 0.6112\n",
      "Epoch 163/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0107 - acc: 0.9966 - val_loss: 3.7657 - val_acc: 0.6038\n",
      "Epoch 164/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0076 - acc: 0.9973 - val_loss: 3.8613 - val_acc: 0.6029\n",
      "Epoch 165/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 3.8197 - val_acc: 0.6091\n",
      "Epoch 166/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0066 - acc: 0.9980 - val_loss: 3.9636 - val_acc: 0.6029\n",
      "Epoch 167/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0065 - acc: 0.9979 - val_loss: 3.8924 - val_acc: 0.6087\n",
      "Epoch 168/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 3.7181 - val_acc: 0.6101\n",
      "Epoch 169/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0116 - acc: 0.9963 - val_loss: 3.6346 - val_acc: 0.6099\n",
      "Epoch 170/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 3.7404 - val_acc: 0.6087\n",
      "Epoch 171/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0045 - acc: 0.9988 - val_loss: 3.8726 - val_acc: 0.6085\n",
      "Epoch 172/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0053 - acc: 0.9983 - val_loss: 3.9352 - val_acc: 0.6082\n",
      "Epoch 173/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 3.7083 - val_acc: 0.6026\n",
      "Epoch 174/200\n",
      "95990/95990 [==============================] - 143s 1ms/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 3.7459 - val_acc: 0.6133\n",
      "Epoch 175/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0056 - acc: 0.9982 - val_loss: 3.8285 - val_acc: 0.6059\n",
      "Epoch 176/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0067 - acc: 0.9978 - val_loss: 3.8697 - val_acc: 0.6121\n",
      "Epoch 177/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0100 - acc: 0.9968 - val_loss: 3.8245 - val_acc: 0.6040\n",
      "Epoch 178/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0124 - acc: 0.9958 - val_loss: 3.7024 - val_acc: 0.6001\n",
      "Epoch 179/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0092 - acc: 0.9971 - val_loss: 3.6717 - val_acc: 0.6128\n",
      "Epoch 180/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0049 - acc: 0.9984 - val_loss: 3.9385 - val_acc: 0.6060\n",
      "Epoch 181/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0050 - acc: 0.9985 - val_loss: 3.9837 - val_acc: 0.5995\n",
      "Epoch 182/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0146 - acc: 0.9952 - val_loss: 3.6819 - val_acc: 0.6118\n",
      "Epoch 183/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 3.7067 - val_acc: 0.6101\n",
      "Epoch 184/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0050 - acc: 0.9984 - val_loss: 3.8465 - val_acc: 0.6073\n",
      "Epoch 185/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 3.9558 - val_acc: 0.6105\n",
      "Epoch 186/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0078 - acc: 0.9975 - val_loss: 3.8234 - val_acc: 0.6103\n",
      "Epoch 187/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0117 - acc: 0.9960 - val_loss: 3.6512 - val_acc: 0.6112\n",
      "Epoch 188/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 3.8730 - val_acc: 0.6055\n",
      "Epoch 189/200\n",
      "95990/95990 [==============================] - 141s 1ms/sample - loss: 0.0072 - acc: 0.9975 - val_loss: 3.8418 - val_acc: 0.6118\n",
      "Epoch 190/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 3.6383 - val_acc: 0.6171\n",
      "Epoch 191/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0081 - acc: 0.9974 - val_loss: 3.7517 - val_acc: 0.6132\n",
      "Epoch 192/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 3.8069 - val_acc: 0.6138\n",
      "Epoch 193/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 3.9398 - val_acc: 0.6088\n",
      "Epoch 194/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0069 - acc: 0.9979 - val_loss: 3.8484 - val_acc: 0.6080\n",
      "Epoch 195/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 3.7674 - val_acc: 0.6088\n",
      "Epoch 196/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 3.8129 - val_acc: 0.6108\n",
      "Epoch 197/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0050 - acc: 0.9983 - val_loss: 3.9707 - val_acc: 0.6090\n",
      "Epoch 198/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0102 - acc: 0.9966 - val_loss: 3.7674 - val_acc: 0.6056\n",
      "Epoch 199/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0079 - acc: 0.9975 - val_loss: 3.7615 - val_acc: 0.6156\n",
      "Epoch 200/200\n",
      "95990/95990 [==============================] - 142s 1ms/sample - loss: 0.0061 - acc: 0.9980 - val_loss: 3.7488 - val_acc: 0.6182\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=200,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history5.history['acc'], label='train_acc')\n",
    "plt.plot(history5.history[\"val_acc\"], label='validation_acc')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_14 (Averag (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 67, 40)            2600      \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_15 (Averag (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 2,529,769\n",
      "Trainable params: 52,669\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model6 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.21, recurrent_dropout=0.1)),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model6.summary()\n",
    "model6.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/20\n",
      "95990/95990 [==============================] - 78s 812us/sample - loss: 1.1281 - acc: 0.4778 - val_loss: 1.0656 - val_acc: 0.5288\n",
      "Epoch 2/20\n",
      "95990/95990 [==============================] - 74s 775us/sample - loss: 1.0202 - acc: 0.5348 - val_loss: 1.0284 - val_acc: 0.5411\n",
      "Epoch 3/20\n",
      "95990/95990 [==============================] - 79s 825us/sample - loss: 0.9712 - acc: 0.5598 - val_loss: 0.9585 - val_acc: 0.5722\n",
      "Epoch 4/20\n",
      "95990/95990 [==============================] - 81s 844us/sample - loss: 0.9222 - acc: 0.5854 - val_loss: 0.9172 - val_acc: 0.5899\n",
      "Epoch 5/20\n",
      "95990/95990 [==============================] - 85s 885us/sample - loss: 0.8866 - acc: 0.6035 - val_loss: 0.9023 - val_acc: 0.5966\n",
      "Epoch 6/20\n",
      "95990/95990 [==============================] - 83s 868us/sample - loss: 0.8642 - acc: 0.6136 - val_loss: 0.8909 - val_acc: 0.6050\n",
      "Epoch 7/20\n",
      "95990/95990 [==============================] - 83s 862us/sample - loss: 0.8412 - acc: 0.6245 - val_loss: 0.8805 - val_acc: 0.6082\n",
      "Epoch 8/20\n",
      "95990/95990 [==============================] - 80s 836us/sample - loss: 0.8264 - acc: 0.6325 - val_loss: 0.8759 - val_acc: 0.6183\n",
      "Epoch 9/20\n",
      "95990/95990 [==============================] - 77s 800us/sample - loss: 0.8162 - acc: 0.6354 - val_loss: 0.8804 - val_acc: 0.6086\n",
      "Epoch 10/20\n",
      "95990/95990 [==============================] - 76s 793us/sample - loss: 0.8021 - acc: 0.6424 - val_loss: 0.8549 - val_acc: 0.6216\n",
      "Epoch 11/20\n",
      "95990/95990 [==============================] - 77s 797us/sample - loss: 0.7925 - acc: 0.6459 - val_loss: 0.8565 - val_acc: 0.6202\n",
      "Epoch 12/20\n",
      "95990/95990 [==============================] - 85s 887us/sample - loss: 0.7877 - acc: 0.6494 - val_loss: 0.8443 - val_acc: 0.6277\n",
      "Epoch 13/20\n",
      "95990/95990 [==============================] - 79s 821us/sample - loss: 0.7736 - acc: 0.6562 - val_loss: 0.8406 - val_acc: 0.6288\n",
      "Epoch 14/20\n",
      "95990/95990 [==============================] - 79s 819us/sample - loss: 0.7654 - acc: 0.6610 - val_loss: 0.8681 - val_acc: 0.6172\n",
      "Epoch 15/20\n",
      "95990/95990 [==============================] - 75s 778us/sample - loss: 0.7589 - acc: 0.6636 - val_loss: 0.8536 - val_acc: 0.6251\n",
      "Epoch 16/20\n",
      "95990/95990 [==============================] - 76s 791us/sample - loss: 0.7517 - acc: 0.6668 - val_loss: 0.8471 - val_acc: 0.6293\n",
      "Epoch 17/20\n",
      "95990/95990 [==============================] - 80s 836us/sample - loss: 0.7457 - acc: 0.6692 - val_loss: 0.8372 - val_acc: 0.6288\n",
      "Epoch 18/20\n",
      "95990/95990 [==============================] - 74s 776us/sample - loss: 0.7387 - acc: 0.6729 - val_loss: 0.8469 - val_acc: 0.6318\n",
      "Epoch 19/20\n",
      "95990/95990 [==============================] - 77s 806us/sample - loss: 0.7304 - acc: 0.6781 - val_loss: 0.8564 - val_acc: 0.6310\n",
      "Epoch 20/20\n",
      "95990/95990 [==============================] - 76s 790us/sample - loss: 0.7308 - acc: 0.6773 - val_loss: 0.8591 - val_acc: 0.6285\n"
     ]
    }
   ],
   "source": [
    "history6 = model6.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/30\n",
      "95990/95990 [==============================] - 75s 781us/sample - loss: 0.7221 - acc: 0.6807 - val_loss: 0.8479 - val_acc: 0.6340\n",
      "Epoch 2/30\n",
      "95990/95990 [==============================] - 75s 784us/sample - loss: 0.7158 - acc: 0.6835 - val_loss: 0.8414 - val_acc: 0.6331\n",
      "Epoch 3/30\n",
      "95990/95990 [==============================] - 76s 787us/sample - loss: 0.7109 - acc: 0.6870 - val_loss: 0.8674 - val_acc: 0.6189\n",
      "Epoch 4/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.7093 - acc: 0.6876 - val_loss: 0.8614 - val_acc: 0.6271\n",
      "Epoch 5/30\n",
      "95990/95990 [==============================] - 76s 795us/sample - loss: 0.7060 - acc: 0.6901 - val_loss: 0.8550 - val_acc: 0.6276\n",
      "Epoch 6/30\n",
      "95990/95990 [==============================] - 77s 797us/sample - loss: 0.7040 - acc: 0.6913 - val_loss: 0.8554 - val_acc: 0.6305\n",
      "Epoch 7/30\n",
      "95990/95990 [==============================] - 76s 794us/sample - loss: 0.7013 - acc: 0.6920 - val_loss: 0.8578 - val_acc: 0.6311\n",
      "Epoch 8/30\n",
      "95990/95990 [==============================] - 77s 804us/sample - loss: 0.6944 - acc: 0.6961 - val_loss: 0.8825 - val_acc: 0.6218\n",
      "Epoch 9/30\n",
      "95990/95990 [==============================] - 76s 796us/sample - loss: 0.6914 - acc: 0.6972 - val_loss: 0.8523 - val_acc: 0.6321\n",
      "Epoch 10/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.6862 - acc: 0.7020 - val_loss: 0.8673 - val_acc: 0.6230\n",
      "Epoch 11/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.6823 - acc: 0.7005 - val_loss: 0.8673 - val_acc: 0.6263\n",
      "Epoch 12/30\n",
      "95990/95990 [==============================] - 76s 797us/sample - loss: 0.6819 - acc: 0.7005 - val_loss: 0.8615 - val_acc: 0.6329\n",
      "Epoch 13/30\n",
      "95990/95990 [==============================] - 77s 797us/sample - loss: 0.6822 - acc: 0.7023 - val_loss: 0.8795 - val_acc: 0.6194\n",
      "Epoch 14/30\n",
      "95990/95990 [==============================] - 77s 797us/sample - loss: 0.6775 - acc: 0.7041 - val_loss: 0.8747 - val_acc: 0.6283\n",
      "Epoch 15/30\n",
      "95990/95990 [==============================] - 76s 795us/sample - loss: 0.6740 - acc: 0.7049 - val_loss: 0.8656 - val_acc: 0.6271\n",
      "Epoch 16/30\n",
      "95990/95990 [==============================] - 77s 802us/sample - loss: 0.6709 - acc: 0.7054 - val_loss: 0.8716 - val_acc: 0.6283\n",
      "Epoch 17/30\n",
      "95990/95990 [==============================] - 76s 794us/sample - loss: 0.6676 - acc: 0.7082 - val_loss: 0.8541 - val_acc: 0.6333\n",
      "Epoch 18/30\n",
      "95990/95990 [==============================] - 76s 795us/sample - loss: 0.6637 - acc: 0.7112 - val_loss: 0.8614 - val_acc: 0.6326\n",
      "Epoch 19/30\n",
      "95990/95990 [==============================] - 76s 794us/sample - loss: 0.6627 - acc: 0.7107 - val_loss: 0.8674 - val_acc: 0.6299\n",
      "Epoch 20/30\n",
      "95990/95990 [==============================] - 77s 797us/sample - loss: 0.6578 - acc: 0.7145 - val_loss: 0.8761 - val_acc: 0.6329\n",
      "Epoch 21/30\n",
      "95990/95990 [==============================] - 76s 796us/sample - loss: 0.6595 - acc: 0.7133 - val_loss: 0.8789 - val_acc: 0.6275\n",
      "Epoch 22/30\n",
      "95990/95990 [==============================] - 77s 805us/sample - loss: 0.6536 - acc: 0.7168 - val_loss: 0.8667 - val_acc: 0.6351\n",
      "Epoch 23/30\n",
      "95990/95990 [==============================] - 76s 788us/sample - loss: 0.6552 - acc: 0.7151 - val_loss: 0.8813 - val_acc: 0.6277\n",
      "Epoch 24/30\n",
      "95990/95990 [==============================] - 75s 782us/sample - loss: 0.6454 - acc: 0.7173 - val_loss: 0.8724 - val_acc: 0.6294\n",
      "Epoch 25/30\n",
      "95990/95990 [==============================] - 76s 787us/sample - loss: 0.6487 - acc: 0.7169 - val_loss: 0.8766 - val_acc: 0.6323\n",
      "Epoch 26/30\n",
      "95990/95990 [==============================] - 75s 783us/sample - loss: 0.6460 - acc: 0.7202 - val_loss: 0.8667 - val_acc: 0.6339\n",
      "Epoch 27/30\n",
      "95990/95990 [==============================] - 76s 790us/sample - loss: 0.6408 - acc: 0.7203 - val_loss: 0.8774 - val_acc: 0.6330\n",
      "Epoch 28/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.6394 - acc: 0.7227 - val_loss: 0.9091 - val_acc: 0.6177\n",
      "Epoch 29/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.6387 - acc: 0.7223 - val_loss: 0.8764 - val_acc: 0.6282\n",
      "Epoch 30/30\n",
      "95990/95990 [==============================] - 76s 789us/sample - loss: 0.6397 - acc: 0.7226 - val_loss: 0.8812 - val_acc: 0.6316\n"
     ]
    }
   ],
   "source": [
    "history6 = model6.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jy/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/jy/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 67, 40)            2600      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,529,703\n",
      "Trainable params: 52,603\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model7 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.21, recurrent_dropout=0.1)),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model7.summary()\n",
    "model7.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/25\n",
      "95990/95990 [==============================] - 77s 798us/sample - loss: 0.9138 - acc: 0.5428 - val_loss: 0.8684 - val_acc: 0.5790\n",
      "Epoch 2/25\n",
      "95990/95990 [==============================] - 74s 770us/sample - loss: 0.8353 - acc: 0.5969 - val_loss: 0.7723 - val_acc: 0.6332\n",
      "Epoch 3/25\n",
      "95990/95990 [==============================] - 74s 770us/sample - loss: 0.7820 - acc: 0.6259 - val_loss: 0.7408 - val_acc: 0.6508\n",
      "Epoch 4/25\n",
      "95990/95990 [==============================] - 74s 766us/sample - loss: 0.7510 - acc: 0.6438 - val_loss: 0.7145 - val_acc: 0.6648\n",
      "Epoch 5/25\n",
      "95990/95990 [==============================] - 74s 769us/sample - loss: 0.7298 - acc: 0.6552 - val_loss: 0.7095 - val_acc: 0.6656\n",
      "Epoch 6/25\n",
      "95990/95990 [==============================] - 74s 776us/sample - loss: 0.7135 - acc: 0.6653 - val_loss: 0.7319 - val_acc: 0.6547\n",
      "Epoch 7/25\n",
      "95990/95990 [==============================] - 92s 961us/sample - loss: 0.6961 - acc: 0.6747 - val_loss: 0.6978 - val_acc: 0.6730\n",
      "Epoch 8/25\n",
      "95990/95990 [==============================] - 106s 1ms/sample - loss: 0.6847 - acc: 0.6816 - val_loss: 0.6843 - val_acc: 0.6813\n",
      "Epoch 9/25\n",
      "95990/95990 [==============================] - 80s 831us/sample - loss: 0.6730 - acc: 0.6868 - val_loss: 0.6982 - val_acc: 0.6758\n",
      "Epoch 10/25\n",
      "95990/95990 [==============================] - 77s 798us/sample - loss: 0.6666 - acc: 0.6896 - val_loss: 0.7168 - val_acc: 0.6621\n",
      "Epoch 11/25\n",
      "95990/95990 [==============================] - 78s 807us/sample - loss: 0.6546 - acc: 0.6975 - val_loss: 0.7264 - val_acc: 0.6673\n",
      "Epoch 12/25\n",
      "95990/95990 [==============================] - 78s 817us/sample - loss: 0.6455 - acc: 0.7022 - val_loss: 0.6793 - val_acc: 0.6849\n",
      "Epoch 13/25\n",
      "95990/95990 [==============================] - 81s 845us/sample - loss: 0.6400 - acc: 0.7059 - val_loss: 0.6780 - val_acc: 0.6863\n",
      "Epoch 14/25\n",
      "95990/95990 [==============================] - 88s 920us/sample - loss: 0.6361 - acc: 0.7072 - val_loss: 0.7176 - val_acc: 0.6714\n",
      "Epoch 15/25\n",
      "95990/95990 [==============================] - 82s 854us/sample - loss: 0.6304 - acc: 0.7096 - val_loss: 0.6823 - val_acc: 0.6869\n",
      "Epoch 16/25\n",
      "95990/95990 [==============================] - 83s 863us/sample - loss: 0.6215 - acc: 0.7155 - val_loss: 0.7011 - val_acc: 0.6796\n",
      "Epoch 17/25\n",
      "95990/95990 [==============================] - 81s 839us/sample - loss: 0.6132 - acc: 0.7213 - val_loss: 0.6761 - val_acc: 0.6894\n",
      "Epoch 18/25\n",
      "95990/95990 [==============================] - 80s 832us/sample - loss: 0.6060 - acc: 0.7262 - val_loss: 0.6752 - val_acc: 0.6894\n",
      "Epoch 19/25\n",
      "95990/95990 [==============================] - 81s 847us/sample - loss: 0.6071 - acc: 0.7251 - val_loss: 0.6737 - val_acc: 0.6891\n",
      "Epoch 20/25\n",
      "95990/95990 [==============================] - 81s 842us/sample - loss: 0.6004 - acc: 0.7270 - val_loss: 0.6798 - val_acc: 0.6901\n",
      "Epoch 21/25\n",
      "95990/95990 [==============================] - 78s 817us/sample - loss: 0.5970 - acc: 0.7286 - val_loss: 0.6781 - val_acc: 0.6878\n",
      "Epoch 22/25\n",
      "95990/95990 [==============================] - 82s 854us/sample - loss: 0.5923 - acc: 0.7320 - val_loss: 0.6803 - val_acc: 0.6901\n",
      "Epoch 23/25\n",
      "95990/95990 [==============================] - 99s 1ms/sample - loss: 0.5905 - acc: 0.7327 - val_loss: 0.6894 - val_acc: 0.6856\n",
      "Epoch 24/25\n",
      "95990/95990 [==============================] - 85s 887us/sample - loss: 0.5867 - acc: 0.7347 - val_loss: 0.7042 - val_acc: 0.6808\n",
      "Epoch 25/25\n",
      "95990/95990 [==============================] - 81s 845us/sample - loss: 0.5771 - acc: 0.7393 - val_loss: 0.7127 - val_acc: 0.6736\n"
     ]
    }
   ],
   "source": [
    "history7 = model7.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history7.history['acc'], label='train_acc')\n",
    "plt.plot(history7.history[\"val_acc\"], label='validation_acc')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/25\n",
      "95990/95990 [==============================] - 77s 805us/sample - loss: 0.5742 - acc: 0.7410 - val_loss: 0.6923 - val_acc: 0.6876\n",
      "Epoch 2/25\n",
      "95990/95990 [==============================] - 78s 810us/sample - loss: 0.5687 - acc: 0.7431 - val_loss: 0.6846 - val_acc: 0.6838\n",
      "Epoch 3/25\n",
      "95990/95990 [==============================] - 77s 804us/sample - loss: 0.5657 - acc: 0.7472 - val_loss: 0.6913 - val_acc: 0.6848\n",
      "Epoch 4/25\n",
      "95990/95990 [==============================] - 77s 806us/sample - loss: 0.5647 - acc: 0.7472 - val_loss: 0.7458 - val_acc: 0.6644\n",
      "Epoch 5/25\n",
      "95990/95990 [==============================] - 82s 849us/sample - loss: 0.5639 - acc: 0.7465 - val_loss: 0.6976 - val_acc: 0.6843\n",
      "Epoch 6/25\n",
      "95990/95990 [==============================] - 85s 884us/sample - loss: 0.5573 - acc: 0.7507 - val_loss: 0.6929 - val_acc: 0.6851\n",
      "Epoch 7/25\n",
      "95990/95990 [==============================] - 83s 867us/sample - loss: 0.5564 - acc: 0.7512 - val_loss: 0.7047 - val_acc: 0.6837\n",
      "Epoch 8/25\n",
      "95990/95990 [==============================] - 83s 863us/sample - loss: 0.5521 - acc: 0.7536 - val_loss: 0.7102 - val_acc: 0.6831\n",
      "Epoch 9/25\n",
      "33024/95990 [=========>....................] - ETA: 48s - loss: 0.5506 - acc: 0.7525"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-49fa416bc7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history7 = model7.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_4 (Average (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 67, 40)            2600      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_5 (Average (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,529,637\n",
      "Trainable params: 52,537\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model8 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.21, recurrent_dropout=0.1)),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model8.summary()\n",
    "model8.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "WARNING:tensorflow:From /Users/jy/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "95990/95990 [==============================] - 81s 849us/sample - loss: 0.5816 - acc: 0.6909 - val_loss: 0.5237 - val_acc: 0.7399\n",
      "Epoch 2/25\n",
      "95990/95990 [==============================] - 80s 832us/sample - loss: 0.5245 - acc: 0.7362 - val_loss: 0.4725 - val_acc: 0.7756\n",
      "Epoch 3/25\n",
      "95990/95990 [==============================] - 81s 841us/sample - loss: 0.4966 - acc: 0.7570 - val_loss: 0.4713 - val_acc: 0.7730\n",
      "Epoch 4/25\n",
      "95990/95990 [==============================] - 78s 808us/sample - loss: 0.4653 - acc: 0.7790 - val_loss: 0.4402 - val_acc: 0.7904\n",
      "Epoch 5/25\n",
      "95990/95990 [==============================] - 76s 792us/sample - loss: 0.4520 - acc: 0.7860 - val_loss: 0.4382 - val_acc: 0.7919\n",
      "Epoch 6/25\n",
      "95990/95990 [==============================] - 76s 796us/sample - loss: 0.4344 - acc: 0.7973 - val_loss: 0.4476 - val_acc: 0.7918\n",
      "Epoch 7/25\n",
      "95990/95990 [==============================] - 77s 805us/sample - loss: 0.4248 - acc: 0.8019 - val_loss: 0.4244 - val_acc: 0.8012\n",
      "Epoch 8/25\n",
      "95990/95990 [==============================] - 82s 856us/sample - loss: 0.4145 - acc: 0.8075 - val_loss: 0.4197 - val_acc: 0.8059\n",
      "Epoch 9/25\n",
      "95990/95990 [==============================] - 79s 819us/sample - loss: 0.4050 - acc: 0.8142 - val_loss: 0.4155 - val_acc: 0.8045\n",
      "Epoch 10/25\n",
      "95990/95990 [==============================] - 81s 844us/sample - loss: 0.3971 - acc: 0.8179 - val_loss: 0.4381 - val_acc: 0.7969\n",
      "Epoch 11/25\n",
      "95990/95990 [==============================] - 80s 829us/sample - loss: 0.3891 - acc: 0.8226 - val_loss: 0.4326 - val_acc: 0.8014\n",
      "Epoch 12/25\n",
      "95990/95990 [==============================] - 83s 864us/sample - loss: 0.3855 - acc: 0.8251 - val_loss: 0.4149 - val_acc: 0.8078\n",
      "Epoch 13/25\n",
      "95990/95990 [==============================] - 81s 840us/sample - loss: 0.3752 - acc: 0.8306 - val_loss: 0.4123 - val_acc: 0.8101\n",
      "Epoch 14/25\n",
      "95990/95990 [==============================] - 79s 828us/sample - loss: 0.3681 - acc: 0.8345 - val_loss: 0.4158 - val_acc: 0.8078\n",
      "Epoch 15/25\n",
      "95990/95990 [==============================] - 81s 842us/sample - loss: 0.3646 - acc: 0.8360 - val_loss: 0.4219 - val_acc: 0.8084\n",
      "Epoch 16/25\n",
      "95990/95990 [==============================] - 78s 815us/sample - loss: 0.3561 - acc: 0.8408 - val_loss: 0.4188 - val_acc: 0.8092\n",
      "Epoch 17/25\n",
      "95990/95990 [==============================] - 76s 791us/sample - loss: 0.3550 - acc: 0.8409 - val_loss: 0.4230 - val_acc: 0.8064\n",
      "Epoch 18/25\n",
      "95990/95990 [==============================] - 78s 815us/sample - loss: 0.3485 - acc: 0.8444 - val_loss: 0.4250 - val_acc: 0.8090\n",
      "Epoch 19/25\n",
      "95990/95990 [==============================] - 77s 807us/sample - loss: 0.3446 - acc: 0.8460 - val_loss: 0.4266 - val_acc: 0.8065\n",
      "Epoch 20/25\n",
      "95990/95990 [==============================] - 84s 873us/sample - loss: 0.3418 - acc: 0.8477 - val_loss: 0.4338 - val_acc: 0.8067\n",
      "Epoch 21/25\n",
      "95990/95990 [==============================] - 88s 922us/sample - loss: 0.3321 - acc: 0.8533 - val_loss: 0.4249 - val_acc: 0.8083\n",
      "Epoch 22/25\n",
      "95990/95990 [==============================] - 80s 836us/sample - loss: 0.3288 - acc: 0.8546 - val_loss: 0.4420 - val_acc: 0.7991\n",
      "Epoch 23/25\n",
      "95990/95990 [==============================] - 79s 819us/sample - loss: 0.3269 - acc: 0.8557 - val_loss: 0.4373 - val_acc: 0.8035\n",
      "Epoch 24/25\n",
      "95990/95990 [==============================] - 81s 840us/sample - loss: 0.3233 - acc: 0.8584 - val_loss: 0.4339 - val_acc: 0.8052\n",
      "Epoch 25/25\n",
      "95990/95990 [==============================] - 79s 825us/sample - loss: 0.3222 - acc: 0.8581 - val_loss: 0.4449 - val_acc: 0.8014\n"
     ]
    }
   ],
   "source": [
    "history8 = model8.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_6 (Average (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 67, 40)            2600      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_7 (Average (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,522,341\n",
      "Trainable params: 45,241\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model9 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32, dropout=0.21, recurrent_dropout=0.1),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model9.summary()\n",
    "model9.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/20\n",
      "95990/95990 [==============================] - 85s 885us/sample - loss: 0.6498 - acc: 0.5992 - val_loss: 0.5675 - val_acc: 0.7067\n",
      "Epoch 2/20\n",
      "95990/95990 [==============================] - 86s 901us/sample - loss: 0.5653 - acc: 0.7082 - val_loss: 0.5382 - val_acc: 0.7294\n",
      "Epoch 3/20\n",
      "95990/95990 [==============================] - 73s 756us/sample - loss: 0.5166 - acc: 0.7444 - val_loss: 0.4761 - val_acc: 0.7710\n",
      "Epoch 4/20\n",
      "95990/95990 [==============================] - 74s 768us/sample - loss: 0.4867 - acc: 0.7629 - val_loss: 0.4855 - val_acc: 0.7701\n",
      "Epoch 5/20\n",
      "95990/95990 [==============================] - 71s 735us/sample - loss: 0.4654 - acc: 0.7766 - val_loss: 0.4396 - val_acc: 0.7932\n",
      "Epoch 6/20\n",
      "95990/95990 [==============================] - 72s 746us/sample - loss: 0.4500 - acc: 0.7864 - val_loss: 0.4356 - val_acc: 0.7935\n",
      "Epoch 7/20\n",
      "95990/95990 [==============================] - 73s 758us/sample - loss: 0.4407 - acc: 0.7928 - val_loss: 0.4286 - val_acc: 0.7984\n",
      "Epoch 8/20\n",
      "95990/95990 [==============================] - 71s 736us/sample - loss: 0.4327 - acc: 0.7970 - val_loss: 0.4247 - val_acc: 0.7995\n",
      "Epoch 9/20\n",
      "95990/95990 [==============================] - 73s 757us/sample - loss: 0.4214 - acc: 0.8029 - val_loss: 0.4196 - val_acc: 0.8024\n",
      "Epoch 10/20\n",
      "95990/95990 [==============================] - 72s 749us/sample - loss: 0.4111 - acc: 0.8081 - val_loss: 0.4198 - val_acc: 0.8055\n",
      "Epoch 11/20\n",
      "95990/95990 [==============================] - 74s 769us/sample - loss: 0.4050 - acc: 0.8112 - val_loss: 0.4195 - val_acc: 0.8052\n",
      "Epoch 12/20\n",
      "95990/95990 [==============================] - 73s 765us/sample - loss: 0.4014 - acc: 0.8145 - val_loss: 0.4326 - val_acc: 0.7920\n",
      "Epoch 13/20\n",
      "95990/95990 [==============================] - 83s 869us/sample - loss: 0.3937 - acc: 0.8189 - val_loss: 0.4168 - val_acc: 0.8038\n",
      "Epoch 14/20\n",
      "95990/95990 [==============================] - 74s 769us/sample - loss: 0.3946 - acc: 0.8195 - val_loss: 0.4198 - val_acc: 0.8077\n",
      "Epoch 15/20\n",
      "95990/95990 [==============================] - 74s 769us/sample - loss: 0.3818 - acc: 0.8253 - val_loss: 0.4334 - val_acc: 0.8026\n",
      "Epoch 16/20\n",
      "95990/95990 [==============================] - 75s 784us/sample - loss: 0.3796 - acc: 0.8270 - val_loss: 0.4256 - val_acc: 0.8059\n",
      "Epoch 17/20\n",
      "95990/95990 [==============================] - 72s 753us/sample - loss: 0.3765 - acc: 0.8281 - val_loss: 0.4187 - val_acc: 0.8072\n",
      "Epoch 18/20\n",
      "14080/95990 [===>..........................] - ETA: 56s - loss: 0.3728 - acc: 0.8321"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5c54482176a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history9 = model9.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 134, 150)          129000    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_18 (Averag (None, 67, 150)           0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 67, 150)           0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 67, 40)            6040      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_19 (Averag (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 64)                20736     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,670,269\n",
      "Trainable params: 193,169\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model11 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.LSTM(150, dropout=0.2, recurrent_dropout=0.1, return_sequences=True),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(64, dropout=0.2, recurrent_dropout=0.1, return_sequences=False),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = optimizers.Adam(lr=0.0001, decay=0.0001);\n",
    "\n",
    "model11.summary()\n",
    "model11.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.6163 - acc: 0.6517 - val_loss: 0.5650 - val_acc: 0.7041\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 579s 6ms/sample - loss: 0.5632 - acc: 0.7110 - val_loss: 0.5339 - val_acc: 0.7320\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 576s 6ms/sample - loss: 0.5469 - acc: 0.7227 - val_loss: 0.5156 - val_acc: 0.7461\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 578s 6ms/sample - loss: 0.5333 - acc: 0.7330 - val_loss: 0.5077 - val_acc: 0.7518\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 580s 6ms/sample - loss: 0.5245 - acc: 0.7392 - val_loss: 0.4936 - val_acc: 0.7590\n",
      "Epoch 6/10\n",
      "95990/95990 [==============================] - 578s 6ms/sample - loss: 0.5143 - acc: 0.7454 - val_loss: 0.4902 - val_acc: 0.7631\n",
      "Epoch 7/10\n",
      "95990/95990 [==============================] - 588s 6ms/sample - loss: 0.5057 - acc: 0.7513 - val_loss: 0.4785 - val_acc: 0.7687\n",
      "Epoch 8/10\n",
      "95990/95990 [==============================] - 593s 6ms/sample - loss: 0.4987 - acc: 0.7576 - val_loss: 0.4784 - val_acc: 0.7706\n",
      "Epoch 9/10\n",
      "95990/95990 [==============================] - 588s 6ms/sample - loss: 0.4913 - acc: 0.7614 - val_loss: 0.4656 - val_acc: 0.7772\n",
      "Epoch 10/10\n",
      "95990/95990 [==============================] - 593s 6ms/sample - loss: 0.4864 - acc: 0.7647 - val_loss: 0.4587 - val_acc: 0.7829\n"
     ]
    }
   ],
   "source": [
    "history11 = model11.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/20\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4812 - acc: 0.7681 - val_loss: 0.4545 - val_acc: 0.7839\n",
      "Epoch 2/20\n",
      "95990/95990 [==============================] - 579s 6ms/sample - loss: 0.4755 - acc: 0.7709 - val_loss: 0.4508 - val_acc: 0.7874\n",
      "Epoch 3/20\n",
      "95990/95990 [==============================] - 579s 6ms/sample - loss: 0.4725 - acc: 0.7740 - val_loss: 0.4511 - val_acc: 0.7859\n",
      "Epoch 4/20\n",
      "95990/95990 [==============================] - 581s 6ms/sample - loss: 0.4685 - acc: 0.7747 - val_loss: 0.4479 - val_acc: 0.7872\n",
      "Epoch 5/20\n",
      "95990/95990 [==============================] - 580s 6ms/sample - loss: 0.4648 - acc: 0.7778 - val_loss: 0.4460 - val_acc: 0.7885\n",
      "Epoch 6/20\n",
      "95990/95990 [==============================] - 581s 6ms/sample - loss: 0.4633 - acc: 0.7789 - val_loss: 0.4415 - val_acc: 0.7899\n",
      "Epoch 7/20\n",
      "95990/95990 [==============================] - 581s 6ms/sample - loss: 0.4603 - acc: 0.7813 - val_loss: 0.4375 - val_acc: 0.7936\n",
      "Epoch 8/20\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4587 - acc: 0.7821 - val_loss: 0.4366 - val_acc: 0.7937\n",
      "Epoch 9/20\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4543 - acc: 0.7829 - val_loss: 0.4368 - val_acc: 0.7933\n",
      "Epoch 10/20\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4534 - acc: 0.7858 - val_loss: 0.4397 - val_acc: 0.7921\n",
      "Epoch 11/20\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4500 - acc: 0.7865 - val_loss: 0.4341 - val_acc: 0.7945\n",
      "Epoch 12/20\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4473 - acc: 0.7882 - val_loss: 0.4297 - val_acc: 0.7967\n",
      "Epoch 13/20\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4462 - acc: 0.7893 - val_loss: 0.4319 - val_acc: 0.7949\n",
      "Epoch 14/20\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4444 - acc: 0.7907 - val_loss: 0.4303 - val_acc: 0.7961\n",
      "Epoch 15/20\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4423 - acc: 0.7913 - val_loss: 0.4315 - val_acc: 0.7953\n",
      "Epoch 16/20\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4414 - acc: 0.7911 - val_loss: 0.4266 - val_acc: 0.7991\n",
      "Epoch 17/20\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4404 - acc: 0.7912 - val_loss: 0.4251 - val_acc: 0.7994\n",
      "Epoch 18/20\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4388 - acc: 0.7926 - val_loss: 0.4303 - val_acc: 0.7951\n",
      "Epoch 19/20\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4369 - acc: 0.7940 - val_loss: 0.4295 - val_acc: 0.7961\n",
      "Epoch 20/20\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4367 - acc: 0.7935 - val_loss: 0.4234 - val_acc: 0.8002\n"
     ]
    }
   ],
   "source": [
    "history11 = model11.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 580s 6ms/sample - loss: 0.4343 - acc: 0.7959 - val_loss: 0.4232 - val_acc: 0.8002\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.4338 - acc: 0.7954 - val_loss: 0.4222 - val_acc: 0.8020\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 597s 6ms/sample - loss: 0.4321 - acc: 0.7970 - val_loss: 0.4210 - val_acc: 0.8023\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 590s 6ms/sample - loss: 0.4309 - acc: 0.7968 - val_loss: 0.4261 - val_acc: 0.7984\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 629s 7ms/sample - loss: 0.4300 - acc: 0.7987 - val_loss: 0.4202 - val_acc: 0.8037\n",
      "Epoch 6/10\n",
      "95990/95990 [==============================] - 634s 7ms/sample - loss: 0.4279 - acc: 0.7999 - val_loss: 0.4196 - val_acc: 0.8042\n",
      "Epoch 7/10\n",
      "95990/95990 [==============================] - 628s 7ms/sample - loss: 0.4287 - acc: 0.7989 - val_loss: 0.4198 - val_acc: 0.8037\n",
      "Epoch 8/10\n",
      "95990/95990 [==============================] - 579s 6ms/sample - loss: 0.4282 - acc: 0.7992 - val_loss: 0.4190 - val_acc: 0.8037\n",
      "Epoch 9/10\n",
      "95990/95990 [==============================] - 654s 7ms/sample - loss: 0.4262 - acc: 0.8001 - val_loss: 0.4186 - val_acc: 0.8049\n",
      "Epoch 10/10\n",
      "95990/95990 [==============================] - 640s 7ms/sample - loss: 0.4251 - acc: 0.8008 - val_loss: 0.4184 - val_acc: 0.8051\n"
     ]
    }
   ],
   "source": [
    "history11 = model11.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8021\n",
      "Epoch 00001: val_loss improved from inf to 0.41773, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 670s 7ms/sample - loss: 0.4236 - acc: 0.8021 - val_loss: 0.4177 - val_acc: 0.8040\n",
      "Epoch 2/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8015\n",
      "Epoch 00002: val_loss improved from 0.41773 to 0.41750, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 638s 7ms/sample - loss: 0.4241 - acc: 0.8014 - val_loss: 0.4175 - val_acc: 0.8044\n",
      "Epoch 3/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8024\n",
      "Epoch 00003: val_loss improved from 0.41750 to 0.41697, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 640s 7ms/sample - loss: 0.4222 - acc: 0.8024 - val_loss: 0.4170 - val_acc: 0.8048\n",
      "Epoch 4/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8031\n",
      "Epoch 00004: val_loss did not improve from 0.41697\n",
      "95990/95990 [==============================] - 589s 6ms/sample - loss: 0.4207 - acc: 0.8031 - val_loss: 0.4178 - val_acc: 0.8037\n",
      "Epoch 5/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8030\n",
      "Epoch 00005: val_loss improved from 0.41697 to 0.41684, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4214 - acc: 0.8030 - val_loss: 0.4168 - val_acc: 0.8054\n",
      "Epoch 6/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8037\n",
      "Epoch 00006: val_loss improved from 0.41684 to 0.41591, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4193 - acc: 0.8036 - val_loss: 0.4159 - val_acc: 0.8062\n",
      "Epoch 7/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4195 - acc: 0.8037\n",
      "Epoch 00007: val_loss did not improve from 0.41591\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4196 - acc: 0.8036 - val_loss: 0.4166 - val_acc: 0.8044\n",
      "Epoch 8/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8045\n",
      "Epoch 00008: val_loss improved from 0.41591 to 0.41527, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4187 - acc: 0.8045 - val_loss: 0.4153 - val_acc: 0.8061\n",
      "Epoch 9/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8049\n",
      "Epoch 00009: val_loss did not improve from 0.41527\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4172 - acc: 0.8049 - val_loss: 0.4157 - val_acc: 0.8053\n",
      "Epoch 10/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8048\n",
      "Epoch 00010: val_loss improved from 0.41527 to 0.41448, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4173 - acc: 0.8048 - val_loss: 0.4145 - val_acc: 0.8065\n",
      "Epoch 11/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8047\n",
      "Epoch 00011: val_loss improved from 0.41448 to 0.41418, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4165 - acc: 0.8048 - val_loss: 0.4142 - val_acc: 0.8064\n",
      "Epoch 12/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8060\n",
      "Epoch 00012: val_loss did not improve from 0.41418\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.4164 - acc: 0.8060 - val_loss: 0.4157 - val_acc: 0.8049\n",
      "Epoch 13/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4163 - acc: 0.8056\n",
      "Epoch 00013: val_loss improved from 0.41418 to 0.41372, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4163 - acc: 0.8056 - val_loss: 0.4137 - val_acc: 0.8065\n",
      "Epoch 14/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8058\n",
      "Epoch 00014: val_loss did not improve from 0.41372\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4137 - acc: 0.8058 - val_loss: 0.4138 - val_acc: 0.8068\n",
      "Epoch 15/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8068\n",
      "Epoch 00015: val_loss did not improve from 0.41372\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4138 - acc: 0.8068 - val_loss: 0.4178 - val_acc: 0.8032\n",
      "Epoch 16/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8076\n",
      "Epoch 00016: val_loss did not improve from 0.41372\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4129 - acc: 0.8076 - val_loss: 0.4149 - val_acc: 0.8058\n",
      "Epoch 17/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8083\n",
      "Epoch 00017: val_loss did not improve from 0.41372\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4116 - acc: 0.8083 - val_loss: 0.4145 - val_acc: 0.8058\n",
      "Epoch 18/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8080\n",
      "Epoch 00018: val_loss improved from 0.41372 to 0.41343, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4129 - acc: 0.8079 - val_loss: 0.4134 - val_acc: 0.8072\n",
      "Epoch 19/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8086\n",
      "Epoch 00019: val_loss improved from 0.41343 to 0.41274, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4115 - acc: 0.8085 - val_loss: 0.4127 - val_acc: 0.8072\n",
      "Epoch 20/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8108\n",
      "Epoch 00020: val_loss improved from 0.41274 to 0.41253, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.4093 - acc: 0.8108 - val_loss: 0.4125 - val_acc: 0.8077\n",
      "Epoch 21/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8089\n",
      "Epoch 00021: val_loss did not improve from 0.41253\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4105 - acc: 0.8090 - val_loss: 0.4141 - val_acc: 0.8061\n",
      "Epoch 22/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8091\n",
      "Epoch 00022: val_loss did not improve from 0.41253\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.4105 - acc: 0.8091 - val_loss: 0.4177 - val_acc: 0.8034\n",
      "Epoch 23/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8104\n",
      "Epoch 00023: val_loss improved from 0.41253 to 0.41232, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.4084 - acc: 0.8104 - val_loss: 0.4123 - val_acc: 0.8073\n",
      "Epoch 24/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8090\n",
      "Epoch 00024: val_loss improved from 0.41232 to 0.41222, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.4093 - acc: 0.8090 - val_loss: 0.4122 - val_acc: 0.8081\n",
      "Epoch 25/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8113\n",
      "Epoch 00025: val_loss improved from 0.41222 to 0.41129, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 588s 6ms/sample - loss: 0.4079 - acc: 0.8113 - val_loss: 0.4113 - val_acc: 0.8089\n",
      "Epoch 26/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8110\n",
      "Epoch 00026: val_loss did not improve from 0.41129\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4075 - acc: 0.8109 - val_loss: 0.4142 - val_acc: 0.8069\n",
      "Epoch 27/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8112\n",
      "Epoch 00027: val_loss did not improve from 0.41129\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4077 - acc: 0.8113 - val_loss: 0.4178 - val_acc: 0.8052\n",
      "Epoch 28/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8118\n",
      "Epoch 00028: val_loss improved from 0.41129 to 0.41090, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4065 - acc: 0.8118 - val_loss: 0.4109 - val_acc: 0.8088\n",
      "Epoch 29/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8114\n",
      "Epoch 00029: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4056 - acc: 0.8114 - val_loss: 0.4122 - val_acc: 0.8066\n",
      "Epoch 30/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8114\n",
      "Epoch 00030: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4062 - acc: 0.8114 - val_loss: 0.4125 - val_acc: 0.8070\n",
      "Epoch 31/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8122\n",
      "Epoch 00031: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4042 - acc: 0.8122 - val_loss: 0.4112 - val_acc: 0.8080\n",
      "Epoch 32/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8118\n",
      "Epoch 00032: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4050 - acc: 0.8118 - val_loss: 0.4134 - val_acc: 0.8065\n",
      "Epoch 33/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8118\n",
      "Epoch 00033: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4041 - acc: 0.8117 - val_loss: 0.4116 - val_acc: 0.8087\n",
      "Epoch 34/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8120\n",
      "Epoch 00034: val_loss did not improve from 0.41090\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4043 - acc: 0.8120 - val_loss: 0.4122 - val_acc: 0.8077\n",
      "Epoch 35/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8124\n",
      "Epoch 00035: val_loss improved from 0.41090 to 0.41035, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 582s 6ms/sample - loss: 0.4028 - acc: 0.8124 - val_loss: 0.4104 - val_acc: 0.8086\n",
      "Epoch 36/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8141\n",
      "Epoch 00036: val_loss did not improve from 0.41035\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4031 - acc: 0.8142 - val_loss: 0.4109 - val_acc: 0.8092\n",
      "Epoch 37/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8135\n",
      "Epoch 00037: val_loss improved from 0.41035 to 0.41030, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4031 - acc: 0.8135 - val_loss: 0.4103 - val_acc: 0.8096\n",
      "Epoch 38/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8139\n",
      "Epoch 00038: val_loss did not improve from 0.41030\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.4020 - acc: 0.8139 - val_loss: 0.4109 - val_acc: 0.8079\n",
      "Epoch 39/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8129\n",
      "Epoch 00039: val_loss improved from 0.41030 to 0.40963, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4027 - acc: 0.8129 - val_loss: 0.4096 - val_acc: 0.8088\n",
      "Epoch 40/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8133\n",
      "Epoch 00040: val_loss did not improve from 0.40963\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4015 - acc: 0.8134 - val_loss: 0.4101 - val_acc: 0.8089\n",
      "Epoch 41/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8151\n",
      "Epoch 00041: val_loss improved from 0.40963 to 0.40946, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.4003 - acc: 0.8151 - val_loss: 0.4095 - val_acc: 0.8087\n",
      "Epoch 42/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8146\n",
      "Epoch 00042: val_loss did not improve from 0.40946\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4012 - acc: 0.8146 - val_loss: 0.4106 - val_acc: 0.8081\n",
      "Epoch 43/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8153\n",
      "Epoch 00043: val_loss improved from 0.40946 to 0.40933, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.3993 - acc: 0.8153 - val_loss: 0.4093 - val_acc: 0.8095\n",
      "Epoch 44/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8136\n",
      "Epoch 00044: val_loss did not improve from 0.40933\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4001 - acc: 0.8136 - val_loss: 0.4100 - val_acc: 0.8088\n",
      "Epoch 45/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8150\n",
      "Epoch 00045: val_loss did not improve from 0.40933\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4002 - acc: 0.8150 - val_loss: 0.4124 - val_acc: 0.8081\n",
      "Epoch 46/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8160\n",
      "Epoch 00046: val_loss did not improve from 0.40933\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3993 - acc: 0.8160 - val_loss: 0.4101 - val_acc: 0.8086\n",
      "Epoch 47/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8142\n",
      "Epoch 00047: val_loss did not improve from 0.40933\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.4001 - acc: 0.8142 - val_loss: 0.4094 - val_acc: 0.8094\n",
      "Epoch 48/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8142\n",
      "Epoch 00048: val_loss did not improve from 0.40933\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3994 - acc: 0.8143 - val_loss: 0.4097 - val_acc: 0.8097\n",
      "Epoch 49/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8159\n",
      "Epoch 00049: val_loss improved from 0.40933 to 0.40919, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.3985 - acc: 0.8158 - val_loss: 0.4092 - val_acc: 0.8093\n",
      "Epoch 50/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8175\n",
      "Epoch 00050: val_loss improved from 0.40919 to 0.40909, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 588s 6ms/sample - loss: 0.3976 - acc: 0.8175 - val_loss: 0.4091 - val_acc: 0.8090\n",
      "Epoch 51/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8160\n",
      "Epoch 00051: val_loss improved from 0.40909 to 0.40899, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.3979 - acc: 0.8160 - val_loss: 0.4090 - val_acc: 0.8089\n",
      "Epoch 52/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8168\n",
      "Epoch 00052: val_loss did not improve from 0.40899\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.3969 - acc: 0.8169 - val_loss: 0.4120 - val_acc: 0.8087\n",
      "Epoch 53/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8161\n",
      "Epoch 00053: val_loss did not improve from 0.40899\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.3966 - acc: 0.8162 - val_loss: 0.4091 - val_acc: 0.8098\n",
      "Epoch 54/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8168\n",
      "Epoch 00054: val_loss improved from 0.40899 to 0.40865, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.3964 - acc: 0.8168 - val_loss: 0.4087 - val_acc: 0.8097\n",
      "Epoch 55/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8168\n",
      "Epoch 00055: val_loss did not improve from 0.40865\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.3954 - acc: 0.8168 - val_loss: 0.4091 - val_acc: 0.8094\n",
      "Epoch 56/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8179\n",
      "Epoch 00056: val_loss did not improve from 0.40865\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.3953 - acc: 0.8179 - val_loss: 0.4092 - val_acc: 0.8099\n",
      "Epoch 57/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8176\n",
      "Epoch 00057: val_loss improved from 0.40865 to 0.40850, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.3958 - acc: 0.8176 - val_loss: 0.4085 - val_acc: 0.8102\n",
      "Epoch 58/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8171\n",
      "Epoch 00058: val_loss did not improve from 0.40850\n",
      "95990/95990 [==============================] - 593s 6ms/sample - loss: 0.3954 - acc: 0.8171 - val_loss: 0.4093 - val_acc: 0.8101\n",
      "Epoch 59/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8179\n",
      "Epoch 00059: val_loss improved from 0.40850 to 0.40795, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3947 - acc: 0.8179 - val_loss: 0.4079 - val_acc: 0.8102\n",
      "Epoch 60/60\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8195\n",
      "Epoch 00060: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3938 - acc: 0.8195 - val_loss: 0.4082 - val_acc: 0.8102\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'models')\n",
    "model_name = 'final_model.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "cb_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history11 = model11.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=60,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[cb_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8180\n",
      "Epoch 00001: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.3943 - acc: 0.8179 - val_loss: 0.4094 - val_acc: 0.8097\n",
      "Epoch 2/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8181\n",
      "Epoch 00002: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 583s 6ms/sample - loss: 0.3933 - acc: 0.8181 - val_loss: 0.4084 - val_acc: 0.8100\n",
      "Epoch 3/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8189\n",
      "Epoch 00003: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.3940 - acc: 0.8188 - val_loss: 0.4081 - val_acc: 0.8101\n",
      "Epoch 4/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8183\n",
      "Epoch 00004: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.3924 - acc: 0.8183 - val_loss: 0.4084 - val_acc: 0.8102\n",
      "Epoch 5/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8188\n",
      "Epoch 00005: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 584s 6ms/sample - loss: 0.3933 - acc: 0.8188 - val_loss: 0.4090 - val_acc: 0.8099\n",
      "Epoch 6/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8176\n",
      "Epoch 00006: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3925 - acc: 0.8176 - val_loss: 0.4084 - val_acc: 0.8107\n",
      "Epoch 7/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8191\n",
      "Epoch 00007: val_loss did not improve from 0.40795\n",
      "95990/95990 [==============================] - 585s 6ms/sample - loss: 0.3923 - acc: 0.8191 - val_loss: 0.4093 - val_acc: 0.8103\n",
      "Epoch 8/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8192\n",
      "Epoch 00008: val_loss improved from 0.40795 to 0.40754, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models/final_model.h5\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3924 - acc: 0.8191 - val_loss: 0.4075 - val_acc: 0.8096\n",
      "Epoch 9/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8180\n",
      "Epoch 00009: val_loss did not improve from 0.40754\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.3933 - acc: 0.8180 - val_loss: 0.4077 - val_acc: 0.8103\n",
      "Epoch 10/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8204\n",
      "Epoch 00010: val_loss did not improve from 0.40754\n",
      "95990/95990 [==============================] - 587s 6ms/sample - loss: 0.3907 - acc: 0.8204 - val_loss: 0.4090 - val_acc: 0.8107\n",
      "Epoch 11/20\n",
      "95936/95990 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8197\n",
      "Epoch 00011: val_loss did not improve from 0.40754\n",
      "95990/95990 [==============================] - 586s 6ms/sample - loss: 0.3910 - acc: 0.8197 - val_loss: 0.4079 - val_acc: 0.8101\n",
      "Epoch 12/20\n",
      "49536/95990 [==============>...............] - ETA: 4:35 - loss: 0.3917 - acc: 0.8198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b7c9feb94ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history11 = model11.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[cb_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 134, 64)           32064     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_16 (Averag (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 67, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 67, 40)            2600      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 63, 16)            3216      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_17 (Averag (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 31, 16)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               148480    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,671,717\n",
      "Trainable params: 194,617\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model12 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(40, activation='tanh'),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.AveragePooling1D(2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(128, dropout=0.21, recurrent_dropout=0.1)),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model12.summary()\n",
    "model12.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95990/95990 [==============================] - 193s 2ms/sample - loss: 0.5674 - acc: 0.7035 - val_loss: 0.5128 - val_acc: 0.7491\n",
      "Epoch 2/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.4994 - acc: 0.7566 - val_loss: 0.4556 - val_acc: 0.7870\n",
      "Epoch 3/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.4682 - acc: 0.7753 - val_loss: 0.4406 - val_acc: 0.7926\n",
      "Epoch 4/10\n",
      "95990/95990 [==============================] - 182s 2ms/sample - loss: 0.4478 - acc: 0.7884 - val_loss: 0.4317 - val_acc: 0.7968\n",
      "Epoch 5/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.4337 - acc: 0.7969 - val_loss: 0.4246 - val_acc: 0.8011\n",
      "Epoch 6/10\n",
      "95990/95990 [==============================] - 183s 2ms/sample - loss: 0.4203 - acc: 0.8046 - val_loss: 0.4403 - val_acc: 0.7948\n",
      "Epoch 7/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.4083 - acc: 0.8108 - val_loss: 0.4710 - val_acc: 0.7718\n",
      "Epoch 8/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.3976 - acc: 0.8164 - val_loss: 0.4236 - val_acc: 0.8043\n",
      "Epoch 9/10\n",
      "95990/95990 [==============================] - 184s 2ms/sample - loss: 0.3892 - acc: 0.8210 - val_loss: 0.4297 - val_acc: 0.8055\n",
      "Epoch 10/10\n",
      "95990/95990 [==============================] - 183s 2ms/sample - loss: 0.3808 - acc: 0.8261 - val_loss: 0.4389 - val_acc: 0.7963\n"
     ]
    }
   ],
   "source": [
    "history12 = model12.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 138, 100)          2477100   \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 138, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 69, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 69, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 69, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128)               41472     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 2,563,873\n",
      "Trainable params: 86,773\n",
      "Non-trainable params: 2,477,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size,\n",
    "    100,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=maxValue,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model13 = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Conv1D(16, 5, activation='relu'),\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model13.summary()\n",
    "model13.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95990 samples, validate on 23998 samples\n",
      "Epoch 1/10\n",
      "95984/95990 [============================>.] - ETA: 0s - loss: -11.6538 - acc: 0.3572\n",
      "Epoch 00001: val_loss improved from inf to -11.96509, saving model to /Users/jy/Desktop/MJU/4-1/DeepLearning/models2/final_model.h5\n",
      "95990/95990 [==============================] - 437s 5ms/sample - loss: -11.6541 - acc: 0.3572 - val_loss: -11.9651 - val_acc: 0.3255\n",
      "Epoch 2/10\n",
      "95984/95990 [============================>.] - ETA: 0s - loss: -11.6538 - acc: 0.3572\n",
      "Epoch 00002: val_loss did not improve from -11.96509\n",
      "95990/95990 [==============================] - 439s 5ms/sample - loss: -11.6541 - acc: 0.3572 - val_loss: -11.9651 - val_acc: 0.3255\n",
      "Epoch 3/10\n",
      "95984/95990 [============================>.] - ETA: 0s - loss: -11.6543 - acc: 0.3572\n",
      "Epoch 00003: val_loss did not improve from -11.96509\n",
      "95990/95990 [==============================] - 441s 5ms/sample - loss: -11.6541 - acc: 0.3572 - val_loss: -11.9651 - val_acc: 0.3255\n",
      "Epoch 4/10\n",
      "16352/95990 [====>.........................] - ETA: 6:18 - loss: -11.6165 - acc: 0.3647"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-c840eca2bc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'models2')\n",
    "model_name = 'final_model.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "cb_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history12 = model12.fit(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[cb_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mju-deeplearning-final",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
